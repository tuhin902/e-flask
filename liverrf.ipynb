{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Importing necessary libraries...\n",
      "Libraries imported successfully.\n",
      "\n",
      "Dataset loaded successfully!\n",
      "First five rows of the dataset:\n",
      "    Age of the patient  Gender of the patient  Total Bilirubin  \\\n",
      "0                65.0                    1.0              0.7   \n",
      "1                62.0                    0.0             10.9   \n",
      "2                62.0                    0.0              7.3   \n",
      "3                58.0                    0.0              1.0   \n",
      "4                72.0                    0.0              3.9   \n",
      "\n",
      "   Direct Bilirubin   Alkphos Alkaline Phosphotase  \\\n",
      "0               0.1                          187.0   \n",
      "1               5.5                          699.0   \n",
      "2               4.1                          490.0   \n",
      "3               0.4                          182.0   \n",
      "4               2.0                          195.0   \n",
      "\n",
      "    Sgpt Alamine Aminotransferase  Sgot Aspartate Aminotransferase  \\\n",
      "0                            16.0                             18.0   \n",
      "1                            64.0                            100.0   \n",
      "2                            60.0                             68.0   \n",
      "3                            14.0                             20.0   \n",
      "4                            27.0                             59.0   \n",
      "\n",
      "   Total Protiens   ALB Albumin  A/G Ratio Albumin and Globulin Ratio  Result  \n",
      "0             6.8           3.3                                  0.90       0  \n",
      "1             7.5           3.2                                  0.74       0  \n",
      "2             7.0           3.3                                  0.89       0  \n",
      "3             6.8           3.4                                  1.00       0  \n",
      "4             7.3           2.4                                  0.40       0   \n",
      "\n",
      "Step 3: Checking for missing values...\n",
      "Missing values per column:\n",
      " Age of the patient                        2\n",
      "Gender of the patient                   754\n",
      "Total Bilirubin                         583\n",
      "Direct Bilirubin                        480\n",
      " Alkphos Alkaline Phosphotase           671\n",
      " Sgpt Alamine Aminotransferase          454\n",
      "Sgot Aspartate Aminotransferase         409\n",
      "Total Protiens                          398\n",
      " ALB Albumin                            430\n",
      "A/G Ratio Albumin and Globulin Ratio    499\n",
      "Result                                    0\n",
      "dtype: int64 \n",
      "\n",
      "Unique values before cleaning: [ 1.  0. nan]\n",
      "\n",
      "Unique values after cleaning: [1 0]\n",
      "\n",
      "Sample of cleaned data:\n",
      "   Gender of the patient\n",
      "0                      1\n",
      "1                      0\n",
      "2                      0\n",
      "3                      0\n",
      "4                      0\n",
      "Step 4: Handling missing values...\n",
      "Missing values handled using imputation.\n",
      "Current dataset shape: (30685, 11) \n",
      "\n",
      "Outliers handled:\n",
      "Original shape: (30685, 11)\n",
      "Imputed shape (outliers imputed): (30685, 11)\n",
      "Step 6: Splitting features and target variable...\n",
      "Features and target split successfully.\n",
      "\n",
      "Step 2: Handling class imbalance with SMOTE...\n",
      "SMOTE applied. The dataset has been balanced.\n",
      "\n",
      "Step 3: Scaling features...\n",
      "Feature scaling applied to resampled data.\n",
      "\n",
      "Step 4: Splitting the data into training and test sets...\n",
      "Train-test split completed.\n",
      "\n",
      "Step 5: Hyperparameter tuning and training the Random Forest model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mm/yhg7s5w52yz6rvpctxf352_c0000gn/T/ipykernel_46753/3230178594.py:46: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].mean(), inplace=True)\n"
     ]
    }
   ],
   "source": [
    "#Step 1: Install and Import Necessary Libraries\n",
    "print(\"Step 1: Importing necessary libraries...\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "print(\"Libraries imported successfully.\\n\")\n",
    "\n",
    "#Step 2: Load the Dataset\n",
    "df = pd.read_csv(\"LPD.csv\",encoding='latin-1')\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(\"First five rows of the dataset:\\n\", df.head(), \"\\n\")\n",
    "\n",
    "\n",
    "# Step 3: Check for Missing Values\n",
    "print(\"Step 3: Checking for missing values...\")\n",
    "print(\"Missing values per column:\\n\", df.isnull().sum(), \"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# Check and convert values in 'Gender of the patient'\n",
    "print(\"Unique values before cleaning:\", df['Gender of the patient'].unique())\n",
    "\n",
    "# Convert any other value to 0 or 1 (example: treat invalid values as 0)\n",
    "df['Gender of the patient'] = df['Gender of the patient'].apply(lambda x: 1 if x == 1 else 0)\n",
    "\n",
    "# Ensure the column is of integer type\n",
    "df['Gender of the patient'] = df['Gender of the patient'].astype(int)\n",
    "\n",
    "print(\"\\nUnique values after cleaning:\", df['Gender of the patient'].unique())\n",
    "print(\"\\nSample of cleaned data:\")\n",
    "print(df[['Gender of the patient']].head())\n",
    "\n",
    "\n",
    "# Step 4: Handling Missing Values\n",
    "print(\"Step 4: Handling missing values...\")\n",
    "\n",
    "# Separate numerical and categorical columns\n",
    "numerical_columns = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Replace missing values for numerical columns with mean\n",
    "for col in numerical_columns:\n",
    "    if df[col].isnull().sum() > 0:\n",
    "        df[col].fillna(df[col].mean(), inplace=True)\n",
    "\n",
    "\n",
    "print(\"Missing values handled using imputation.\")\n",
    "print(\"Current dataset shape:\", df.shape, \"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Step 1: Identify outliers using IQR\n",
    "Q1 = df[numerical_columns].quantile(0.25)\n",
    "Q3 = df[numerical_columns].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define outlier boundaries\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Find rows where any numerical column has values outside the bounds\n",
    "outliers = ((df[numerical_columns] < lower_bound) | (df[numerical_columns] > upper_bound))\n",
    "\n",
    "# Step 2: Handle outliers\n",
    "\n",
    "# Option 1: Impute outliers with the median (for example)\n",
    "df_imputed = df.copy()\n",
    "for col in numerical_columns:\n",
    "    median_value = df[col].median()\n",
    "    df_imputed[col] = np.where((df[col] < lower_bound[col]) | (df[col] > upper_bound[col]), median_value, df[col])\n",
    "\n",
    "# Print confirmation message\n",
    "print(\"Outliers handled:\")\n",
    "print(f\"Original shape: {df.shape}\")\n",
    "print(f\"Imputed shape (outliers imputed): {df_imputed.shape}\")\n",
    "\n",
    "\n",
    "# Step 6: Split Features and Target Variable\n",
    "print(\"Step 6: Splitting features and target variable...\")\n",
    "X = df.drop('Result', axis=1)  # Features\n",
    "y = df['Result']  # Target variable\n",
    "print(\"Features and target split successfully.\\n\")\n",
    "\n",
    "\n",
    "# Step 2: Handle Imbalanced Dataset using SMOTE (Synthetic Minority Over-sampling Technique)\n",
    "print(\"Step 2: Handling class imbalance with SMOTE...\")\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Apply SMOTE to balance the classes\n",
    "smote = SMOTE(random_state=42)  # Random seed for reproducibility\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "print(\"SMOTE applied. The dataset has been balanced.\\n\")\n",
    "\n",
    "\n",
    "# Step 3: Feature Scaling\n",
    "print(\"Step 3: Scaling features...\")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Scale features (important for algorithms sensitive to feature scale like Random Forest)\n",
    "scaler = StandardScaler().fit(X_resampled)  # Fit the scaler on the resampled data\n",
    "X_resampled_scaled = scaler.transform(X_resampled)  # Scale the features\n",
    "print(\"Feature scaling applied to resampled data.\\n\")\n",
    "\n",
    "\n",
    "# Step 4: Train-Test Split\n",
    "print(\"Step 4: Splitting the data into training and test sets...\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the resampled data into training and test sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled_scaled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled)\n",
    "\n",
    "print(\"Train-test split completed.\\n\")\n",
    "\n",
    "\n",
    "# Step 5: Hyperparameter Tuning and Model Training (Random Forest)\n",
    "print(\"Step 5: Hyperparameter tuning and training the Random Forest model...\")\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize the Random Forest model with hyperparameters\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,          # Number of trees\n",
    "    max_depth=10,              # Maximum depth of trees\n",
    "    random_state=42,           # Random seed for reproducibility\n",
    "    class_weight='balanced'    # Handle imbalance by adjusting class weights\n",
    ")\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "# Perform 5-fold cross-validation on the training data\n",
    "cv_scores = cross_val_score(rf_model, X_train, y_train, cv=5, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency of values in 'Gender of the patient':\n",
      "Gender of the patient\n",
      "0    22847\n",
      "1     7838\n",
      "Name: count, dtype: int64\n",
      "\n",
      "The most frequent value in 'Gender of the patient' is: 0 (Count: 22847)\n",
      "Model trained successfully on the balanced dataset.\n",
      "\n",
      "Step 6: Adjusting threshold and making predictions...\n",
      "Accuracy with adjusted threshold: 0.8528103979021776\n",
      "Classification Report with adjusted threshold:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.71      0.83      4386\n",
      "           1       0.77      1.00      0.87      4385\n",
      "\n",
      "    accuracy                           0.85      8771\n",
      "   macro avg       0.89      0.85      0.85      8771\n",
      "weighted avg       0.89      0.85      0.85      8771\n",
      "\n",
      "Confusion Matrix with adjusted threshold:\n",
      " [[3096 1290]\n",
      " [   1 4384]]\n",
      "Model evaluation completed.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Count the frequency of each value in 'Gender of the patient'\n",
    "gender_counts = df['Gender of the patient'].value_counts()\n",
    "\n",
    "# Print the frequencies\n",
    "print(\"Frequency of values in 'Gender of the patient':\")\n",
    "print(gender_counts)\n",
    "\n",
    "# Find and print the most frequent value\n",
    "most_frequent_gender = gender_counts.idxmax()\n",
    "print(f\"\\nThe most frequent value in 'Gender of the patient' is: {most_frequent_gender} (Count: {gender_counts[most_frequent_gender]})\")\n",
    "\n",
    "\n",
    "# Train the Random Forest model on the resampled and scaled training data\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Model trained successfully on the balanced dataset.\\n\")\n",
    "\n",
    "# Step 6: Adjust Classification Threshold and Make Predictions\n",
    "print(\"Step 6: Adjusting threshold and making predictions...\")\n",
    "\n",
    "# Get probabilities of the classes (0 and 1)\n",
    "y_prob = rf_model.predict_proba(X_test)\n",
    "\n",
    "# Set the threshold to 0.3 for predicting class 1 (liver disease)\n",
    "threshold = 0.3\n",
    "y_pred_adjusted = [1 if prob[1] > threshold else 0 for prob in y_prob]\n",
    "\n",
    "# Evaluate the model performance with adjusted threshold\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "print(\"Accuracy with adjusted threshold:\", accuracy_score(y_test, y_pred_adjusted))\n",
    "print(\"Classification Report with adjusted threshold:\\n\", classification_report(y_test, y_pred_adjusted))\n",
    "print(\"Confusion Matrix with adjusted threshold:\\n\", confusion_matrix(y_test, y_pred_adjusted))\n",
    "\n",
    "print(\"Model evaluation completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
