{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Importing necessary libraries...\n",
      "Libraries imported successfully.\n",
      "\n",
      "Step 2: Loading the dataset...\n",
      "Dataset loaded successfully!\n",
      "First five rows of the dataset:\n",
      "    Age of the patient  Gender of the patient  Total Bilirubin  \\\n",
      "0                65.0                    1.0              0.7   \n",
      "1                62.0                    0.0             10.9   \n",
      "2                62.0                    0.0              7.3   \n",
      "3                58.0                    0.0              1.0   \n",
      "4                72.0                    0.0              3.9   \n",
      "\n",
      "   Direct Bilirubin   Alkphos Alkaline Phosphotase  \\\n",
      "0               0.1                          187.0   \n",
      "1               5.5                          699.0   \n",
      "2               4.1                          490.0   \n",
      "3               0.4                          182.0   \n",
      "4               2.0                          195.0   \n",
      "\n",
      "    Sgpt Alamine Aminotransferase  Sgot Aspartate Aminotransferase  \\\n",
      "0                            16.0                             18.0   \n",
      "1                            64.0                            100.0   \n",
      "2                            60.0                             68.0   \n",
      "3                            14.0                             20.0   \n",
      "4                            27.0                             59.0   \n",
      "\n",
      "   Total Protiens   ALB Albumin  A/G Ratio Albumin and Globulin Ratio  Result  \n",
      "0             6.8           3.3                                  0.90       0  \n",
      "1             7.5           3.2                                  0.74       0  \n",
      "2             7.0           3.3                                  0.89       0  \n",
      "3             6.8           3.4                                  1.00       0  \n",
      "4             7.3           2.4                                  0.40       0   \n",
      "\n",
      "Step 3: Checking for missing values...\n",
      "Missing values per column:\n",
      " Age of the patient                        2\n",
      "Gender of the patient                   754\n",
      "Total Bilirubin                         583\n",
      "Direct Bilirubin                        480\n",
      " Alkphos Alkaline Phosphotase           671\n",
      " Sgpt Alamine Aminotransferase          454\n",
      "Sgot Aspartate Aminotransferase         409\n",
      "Total Protiens                          398\n",
      " ALB Albumin                            430\n",
      "A/G Ratio Albumin and Globulin Ratio    499\n",
      "Result                                    0\n",
      "dtype: int64 \n",
      "\n",
      "\n",
      "Step 5.1: Cleaning feature names...\n",
      "Feature names cleaned successfully!\n",
      "Updated feature names:\n",
      " ['Age of the patient', 'Gender of the patient', 'Total Bilirubin', 'Direct Bilirubin', 'Alkphos Alkaline Phosphotase', 'Sgpt Alamine Aminotransferase', 'Sgot Aspartate Aminotransferase', 'Total Protiens', 'ALB Albumin', 'A/G Ratio Albumin and Globulin Ratio', 'Result'] \n",
      "\n",
      "Step 4: Cleaning 'Gender of the patient' column...\n",
      "Unique values before cleaning: [ 1.  0. nan]\n",
      "Unique values after cleaning: [1 0]\n",
      "\n",
      "Sample of cleaned data:\n",
      "   Gender of the patient\n",
      "0                      1\n",
      "1                      0\n",
      "2                      0\n",
      "3                      0\n",
      "4                      0\n",
      "\n",
      "Step 5: Handling missing values...\n",
      "Missing values handled using mean imputation.\n",
      "Current dataset shape: (30685, 11) \n",
      "\n",
      "Step 6: Splitting features and target variable...\n",
      "Features and target split successfully.\n",
      "\n",
      "Step 7: Splitting the data into training and test sets...\n",
      "Train-test split completed.\n",
      "\n",
      "Step 8: Handling class imbalance using SMOTE on training data...\n",
      "SMOTE applied. Training data balanced.\n",
      "\n",
      "Step 9: Scaling features for training data...\n",
      "Feature scaling applied to training data.\n",
      "\n",
      "Step 10: Hyperparameter tuning and cross-validation for XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mm/yhg7s5w52yz6rvpctxf352_c0000gn/T/ipykernel_48353/2012391893.py:41: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].mean(), inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Cross-validation scores: [0.94613083 0.94385065 0.9468358  0.94940137 0.94811859]\n",
      "Mean XGBoost cross-validation score: 0.946867446366204\n",
      "\n",
      "Step 11: Training XGBoost model on training data...\n",
      "XGBoost model trained successfully.\n",
      "\n",
      "Step 12: Analyzing feature importance...\n",
      "Feature importance calculated. Top features:\n",
      "                                Feature  Importance\n",
      "3                      Direct Bilirubin    0.240747\n",
      "1                 Gender of the patient    0.130164\n",
      "5         Sgpt Alamine Aminotransferase    0.100998\n",
      "4          Alkphos Alkaline Phosphotase    0.092869\n",
      "9  A/G Ratio Albumin and Globulin Ratio    0.090229\n",
      "\n",
      "Step 13: Selecting important features for prediction...\n",
      "Selected important features: ['Direct Bilirubin', 'Gender of the patient', 'Sgpt Alamine Aminotransferase', 'Alkphos Alkaline Phosphotase', 'A/G Ratio Albumin and Globulin Ratio']\n",
      "Dataset reduced to important features.\n",
      "\n",
      "Step 14: Splitting features and target variable for important features...\n",
      "Train-test split completed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Install and Import Necessary Libraries\n",
    "print(\"Step 1: Importing necessary libraries...\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "print(\"Libraries imported successfully.\\n\")\n",
    "\n",
    "# Step 2: Load the Dataset\n",
    "print(\"Step 2: Loading the dataset...\")\n",
    "df = pd.read_csv(\"LPD.csv\", encoding='latin-1')\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(\"First five rows of the dataset:\\n\", df.head(), \"\\n\")\n",
    "\n",
    "# Step 3: Check for Missing Values\n",
    "print(\"Step 3: Checking for missing values...\")\n",
    "print(\"Missing values per column:\\n\", df.isnull().sum(), \"\\n\")\n",
    "\n",
    "# Step 5.1: Clean Feature Names\n",
    "print(\"\\nStep 5.1: Cleaning feature names...\")\n",
    "df.columns = df.columns.str.strip().str.replace('\\xa0', '', regex=False)\n",
    "print(\"Feature names cleaned successfully!\")\n",
    "print(\"Updated feature names:\\n\", df.columns.tolist(), \"\\n\")\n",
    "\n",
    "# Step 4: Clean 'Gender of the patient'\n",
    "print(\"Step 4: Cleaning 'Gender of the patient' column...\")\n",
    "print(\"Unique values before cleaning:\", df['Gender of the patient'].unique())\n",
    "df['Gender of the patient'] = df['Gender of the patient'].apply(lambda x: 1 if x == 1 else 0).astype(int)\n",
    "print(\"Unique values after cleaning:\", df['Gender of the patient'].unique())\n",
    "print(\"\\nSample of cleaned data:\")\n",
    "print(df[['Gender of the patient']].head())\n",
    "\n",
    "# Step 5: Handling Missing Values\n",
    "print(\"\\nStep 5: Handling missing values...\")\n",
    "numerical_columns = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "for col in numerical_columns:\n",
    "    if df[col].isnull().sum() > 0:\n",
    "        df[col].fillna(df[col].mean(), inplace=True)\n",
    "print(\"Missing values handled using mean imputation.\")\n",
    "print(\"Current dataset shape:\", df.shape, \"\\n\")\n",
    "\n",
    "# Step 6: Split Features and Target Variable\n",
    "print(\"Step 6: Splitting features and target variable...\")\n",
    "X = df.drop('Result', axis=1)  # Features\n",
    "y = df['Result']  # Target variable\n",
    "print(\"Features and target split successfully.\\n\")\n",
    "\n",
    "# Step 7: Train-Test Split\n",
    "print(\"Step 7: Splitting the data into training and test sets...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "print(\"Train-test split completed.\\n\")\n",
    "\n",
    "# Step 8: Handle Imbalanced Dataset Using SMOTE on Training Data\n",
    "print(\"Step 8: Handling class imbalance using SMOTE on training data...\")\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "print(\"SMOTE applied. Training data balanced.\\n\")\n",
    "\n",
    "# Step 9: Feature Scaling on Training Data Only\n",
    "print(\"Step 9: Scaling features for training data...\")\n",
    "scaler = StandardScaler().fit(X_train_resampled)\n",
    "X_train_resampled_scaled = scaler.transform(X_train_resampled)\n",
    "print(\"Feature scaling applied to training data.\\n\")\n",
    "\n",
    "# Step 10: Hyperparameter Tuning and Cross-Validation on Training Data\n",
    "print(\"Step 10: Hyperparameter tuning and cross-validation for XGBoost...\")\n",
    "\n",
    "xgb_model = XGBClassifier(\n",
    "    max_depth=6,\n",
    "    min_child_weight=5,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    learning_rate=0.01,\n",
    "    n_estimators=500,\n",
    "    reg_alpha=0.1,\n",
    "    reg_lambda=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "cv_scores_xgb = cross_val_score(xgb_model, X_train_resampled_scaled, y_train_resampled, cv=5, scoring='accuracy')\n",
    "print(f\"XGBoost Cross-validation scores: {cv_scores_xgb}\")\n",
    "print(f\"Mean XGBoost cross-validation score: {cv_scores_xgb.mean()}\\n\")\n",
    "\n",
    "# Step 11: Train XGBoost Model on Resampled and Scaled Training Data\n",
    "print(\"Step 11: Training XGBoost model on training data...\")\n",
    "xgb_model.fit(X_train_resampled_scaled, y_train_resampled)\n",
    "print(\"XGBoost model trained successfully.\\n\")\n",
    "\n",
    "# Step 12: Feature Importance Analysis\n",
    "print(\"Step 12: Analyzing feature importance...\")\n",
    "feature_importances = xgb_model.feature_importances_\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': feature_importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "print(\"Feature importance calculated. Top features:\")\n",
    "print(feature_importance_df.head())\n",
    "\n",
    "# Step 13: Select Important Features for Prediction\n",
    "print(\"\\nStep 13: Selecting important features for prediction...\")\n",
    "# Select the top 5 important features (adjust based on your analysis)\n",
    "important_features = feature_importance_df.head(5)['Feature'].tolist()\n",
    "print(f\"Selected important features: {important_features}\")\n",
    "\n",
    "\n",
    "\n",
    "# Create a new dataset with only the important features\n",
    "X_important = df[important_features]  # Use only the selected features\n",
    "print(\"Dataset reduced to important features.\\n\")\n",
    "\n",
    "# Step 14: Split Features and Target Variable\n",
    "print(\"Step 14: Splitting features and target variable for important features...\")\n",
    "X_train_imp, X_test_imp, y_train_imp, y_test_imp = train_test_split(\n",
    "    X_important, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "print(\"Train-test split completed.\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 15: Training XGBoost model with important features...\n",
      "XGBoost model trained successfully with important features.\n",
      "\n",
      "Step 16: Making predictions with important features...\n",
      "\n",
      "Step 17: Evaluating model performance with important features...\n",
      "Accuracy with important features: 0.9287925696594427\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.99      0.95      4385\n",
      "           1       0.97      0.78      0.86      1752\n",
      "\n",
      "    accuracy                           0.93      6137\n",
      "   macro avg       0.94      0.88      0.91      6137\n",
      "weighted avg       0.93      0.93      0.93      6137\n",
      "\n",
      "Confusion Matrix:\n",
      " [[4339   46]\n",
      " [ 391 1361]]\n",
      "Model evaluation with important features completed.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 15: Train XGBoost Model with Important Features\n",
    "print(\"Step 15: Training XGBoost model with important features...\")\n",
    "xgb_model_imp = XGBClassifier(\n",
    "    max_depth=6,\n",
    "    min_child_weight=5,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    learning_rate=0.01,\n",
    "    n_estimators=500,\n",
    "    reg_alpha=0.1,\n",
    "    reg_lambda=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model on the training data with important features\n",
    "xgb_model_imp.fit(X_train_imp, y_train_imp)\n",
    "print(\"XGBoost model trained successfully with important features.\\n\")\n",
    "\n",
    "# Step 16: Make Predictions with Important Features\n",
    "print(\"Step 16: Making predictions with important features...\")\n",
    "y_pred_imp = xgb_model_imp.predict(X_test_imp)\n",
    "\n",
    "\n",
    "\n",
    "# Step 17: Evaluate Model with Important Features\n",
    "print(\"\\nStep 17: Evaluating model performance with important features...\")\n",
    "accuracy_imp = accuracy_score(y_test_imp, y_pred_imp)\n",
    "print(f\"Accuracy with important features: {accuracy_imp}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test_imp, y_pred_imp))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test_imp, y_pred_imp))\n",
    "print(\"Model evaluation with important features completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open ('mlmodel.pkl','wb') as files:\n",
    "    pickle.dump(xgb_model_imp,files)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
