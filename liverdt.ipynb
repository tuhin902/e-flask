{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Importing necessary libraries...\n",
      "Libraries imported successfully.\n",
      "\n",
      "Dataset loaded successfully!\n",
      "First five rows of the dataset:\n",
      "    Age of the patient  Gender of the patient  Total Bilirubin  \\\n",
      "0                65.0                    1.0              0.7   \n",
      "1                62.0                    0.0             10.9   \n",
      "2                62.0                    0.0              7.3   \n",
      "3                58.0                    0.0              1.0   \n",
      "4                72.0                    0.0              3.9   \n",
      "\n",
      "   Direct Bilirubin   Alkphos Alkaline Phosphotase  \\\n",
      "0               0.1                          187.0   \n",
      "1               5.5                          699.0   \n",
      "2               4.1                          490.0   \n",
      "3               0.4                          182.0   \n",
      "4               2.0                          195.0   \n",
      "\n",
      "    Sgpt Alamine Aminotransferase  Sgot Aspartate Aminotransferase  \\\n",
      "0                            16.0                             18.0   \n",
      "1                            64.0                            100.0   \n",
      "2                            60.0                             68.0   \n",
      "3                            14.0                             20.0   \n",
      "4                            27.0                             59.0   \n",
      "\n",
      "   Total Protiens   ALB Albumin  A/G Ratio Albumin and Globulin Ratio  Result  \n",
      "0             6.8           3.3                                  0.90       0  \n",
      "1             7.5           3.2                                  0.74       0  \n",
      "2             7.0           3.3                                  0.89       0  \n",
      "3             6.8           3.4                                  1.00       0  \n",
      "4             7.3           2.4                                  0.40       0   \n",
      "\n",
      "Step 3: Checking for missing values...\n",
      "Missing values per column:\n",
      " Age of the patient                        2\n",
      "Gender of the patient                   754\n",
      "Total Bilirubin                         583\n",
      "Direct Bilirubin                        480\n",
      " Alkphos Alkaline Phosphotase           671\n",
      " Sgpt Alamine Aminotransferase          454\n",
      "Sgot Aspartate Aminotransferase         409\n",
      "Total Protiens                          398\n",
      " ALB Albumin                            430\n",
      "A/G Ratio Albumin and Globulin Ratio    499\n",
      "Result                                    0\n",
      "dtype: int64 \n",
      "\n",
      "Unique values before cleaning: [ 1.  0. nan]\n",
      "\n",
      "Unique values after cleaning: [1 0]\n",
      "\n",
      "Sample of cleaned data:\n",
      "   Gender of the patient\n",
      "0                      1\n",
      "1                      0\n",
      "2                      0\n",
      "3                      0\n",
      "4                      0\n",
      "Step 4: Handling missing values...\n",
      "Missing values handled using imputation.\n",
      "Current dataset shape: (30685, 11) \n",
      "\n",
      "Outliers handled:\n",
      "Original shape: (30685, 11)\n",
      "Imputed shape (outliers imputed): (30685, 11)\n",
      "Step 7: Splitting features and target variable...\n",
      "Features and target split successfully.\n",
      "\n",
      "Step 8: Handling class imbalance with SMOTE...\n",
      "SMOTE applied. The dataset has been balanced.\n",
      "\n",
      "Step 9: Scaling features...\n",
      "Feature scaling applied to resampled data.\n",
      "\n",
      "Step 10: Splitting the data into training and test sets...\n",
      "Train-test split completed.\n",
      "\n",
      "Step 11: Hyperparameter tuning and training the Decision Tree model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mm/yhg7s5w52yz6rvpctxf352_c0000gn/T/ipykernel_46396/555079375.py:42: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].mean(), inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Cross-validation scores: [0.87059997 0.86031927 0.8868301  0.86958381 0.84720639]\n",
      "Mean Decision Tree cross-validation score: 0.8669079076404932\n",
      "Decision Tree model trained successfully on the balanced dataset.\n",
      "\n",
      "Step 12: Making predictions without adjusting the threshold for Decision Tree...\n",
      "Decision Tree Accuracy without adjusted threshold: 0.8801732983696272\n",
      "Decision Tree Classification Report without adjusted threshold:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.84      0.87      4386\n",
      "           1       0.85      0.92      0.89      4385\n",
      "\n",
      "    accuracy                           0.88      8771\n",
      "   macro avg       0.88      0.88      0.88      8771\n",
      "weighted avg       0.88      0.88      0.88      8771\n",
      "\n",
      "Decision Tree Confusion Matrix without adjusted threshold:\n",
      " [[3673  713]\n",
      " [ 338 4047]]\n",
      "Decision Tree model evaluation completed.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Install and Import Necessary Libraries\n",
    "print(\"Step 1: Importing necessary libraries...\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "print(\"Libraries imported successfully.\\n\")\n",
    "\n",
    "# Step 2: Load the Dataset\n",
    "df = pd.read_csv(\"LPD.csv\", encoding='latin-1')\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(\"First five rows of the dataset:\\n\", df.head(), \"\\n\")\n",
    "\n",
    "# Step 3: Check for Missing Values\n",
    "print(\"Step 3: Checking for missing values...\")\n",
    "print(\"Missing values per column:\\n\", df.isnull().sum(), \"\\n\")\n",
    "\n",
    "# Check and convert values in 'Gender of the patient'\n",
    "print(\"Unique values before cleaning:\", df['Gender of the patient'].unique())\n",
    "\n",
    "# Convert any other value to 0 or 1 (example: treat invalid values as 0)\n",
    "df['Gender of the patient'] = df['Gender of the patient'].apply(lambda x: 1 if x == 1 else 0)\n",
    "\n",
    "# Ensure the column is of integer type\n",
    "df['Gender of the patient'] = df['Gender of the patient'].astype(int)\n",
    "\n",
    "print(\"\\nUnique values after cleaning:\", df['Gender of the patient'].unique())\n",
    "print(\"\\nSample of cleaned data:\")\n",
    "print(df[['Gender of the patient']].head())\n",
    "\n",
    "# Step 4: Handling Missing Values\n",
    "print(\"Step 4: Handling missing values...\")\n",
    "\n",
    "# Separate numerical and categorical columns\n",
    "numerical_columns = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Replace missing values for numerical columns with mean\n",
    "for col in numerical_columns:\n",
    "    if df[col].isnull().sum() > 0:\n",
    "        df[col].fillna(df[col].mean(), inplace=True)\n",
    "\n",
    "print(\"Missing values handled using imputation.\")\n",
    "print(\"Current dataset shape:\", df.shape, \"\\n\")\n",
    "\n",
    "# Step 5: Identify outliers using IQR\n",
    "Q1 = df[numerical_columns].quantile(0.25)\n",
    "Q3 = df[numerical_columns].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define outlier boundaries\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Find rows where any numerical column has values outside the bounds\n",
    "outliers = ((df[numerical_columns] < lower_bound) | (df[numerical_columns] > upper_bound))\n",
    "\n",
    "# Step 6: Handle outliers (Imputation with median)\n",
    "df_imputed = df.copy()\n",
    "for col in numerical_columns:\n",
    "    median_value = df[col].median()\n",
    "    df_imputed[col] = np.where((df[col] < lower_bound[col]) | (df[col] > upper_bound[col]), median_value, df[col])\n",
    "\n",
    "# Print confirmation message\n",
    "print(\"Outliers handled:\")\n",
    "print(f\"Original shape: {df.shape}\")\n",
    "print(f\"Imputed shape (outliers imputed): {df_imputed.shape}\")\n",
    "\n",
    "# Step 7: Split Features and Target Variable\n",
    "print(\"Step 7: Splitting features and target variable...\")\n",
    "X = df.drop('Result', axis=1)  # Features\n",
    "y = df['Result']  # Target variable\n",
    "print(\"Features and target split successfully.\\n\")\n",
    "\n",
    "# Step 8: Handle Imbalanced Dataset using SMOTE (Synthetic Minority Over-sampling Technique)\n",
    "print(\"Step 8: Handling class imbalance with SMOTE...\")\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Apply SMOTE to balance the classes\n",
    "smote = SMOTE(random_state=42)  # Random seed for reproducibility\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "print(\"SMOTE applied. The dataset has been balanced.\\n\")\n",
    "\n",
    "# Step 9: Feature Scaling\n",
    "print(\"Step 9: Scaling features...\")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Scale features (important for algorithms sensitive to feature scale like Decision Tree)\n",
    "scaler = StandardScaler().fit(X_resampled)  # Fit the scaler on the resampled data\n",
    "X_resampled_scaled = scaler.transform(X_resampled)  # Scale the features\n",
    "print(\"Feature scaling applied to resampled data.\\n\")\n",
    "\n",
    "# Step 10: Train-Test Split\n",
    "print(\"Step 10: Splitting the data into training and test sets...\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the resampled data into training and test sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled_scaled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled)\n",
    "\n",
    "print(\"Train-test split completed.\\n\")\n",
    "\n",
    "# Step 11: Hyperparameter Tuning and Model Training (Decision Tree)\n",
    "print(\"Step 11: Hyperparameter tuning and training the Decision Tree model...\")\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# Initialize the Decision Tree model\n",
    "dt_model = DecisionTreeClassifier(\n",
    "    max_depth=10,              # Maximum depth of the tree\n",
    "    random_state=42,           # Random seed for reproducibility\n",
    "    class_weight='balanced'    # Handle imbalance by adjusting class weights\n",
    ")\n",
    "\n",
    "# Perform 5-fold cross-validation on the training data\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "cv_scores_dt = cross_val_score(dt_model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation results\n",
    "print(f\"Decision Tree Cross-validation scores: {cv_scores_dt}\")\n",
    "print(f\"Mean Decision Tree cross-validation score: {cv_scores_dt.mean()}\")\n",
    "\n",
    "# Train the Decision Tree model on the resampled and scaled training data\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Decision Tree model trained successfully on the balanced dataset.\\n\")\n",
    "\n",
    "# Step 12: Making Predictions Without Adjusting the Threshold (Decision Tree)\n",
    "print(\"Step 12: Making predictions without adjusting the threshold for Decision Tree...\")\n",
    "\n",
    "# Predict the classes directly (using the default threshold of 0.5)\n",
    "y_pred_dt = dt_model.predict(X_test)\n",
    "\n",
    "# Evaluate the Decision Tree model performance with the default threshold\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "print(\"Decision Tree Accuracy without adjusted threshold:\", accuracy_score(y_test, y_pred_dt))\n",
    "print(\"Decision Tree Classification Report without adjusted threshold:\\n\", classification_report(y_test, y_pred_dt))\n",
    "print(\"Decision Tree Confusion Matrix without adjusted threshold:\\n\", confusion_matrix(y_test, y_pred_dt))\n",
    "\n",
    "print(\"Decision Tree model evaluation completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('dt_model.pkl','wb') as files:\n",
    "    pickle.dump(dt_model,files)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
